{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group 8 Assignment Phase 3 : Gradient Boosted Decision Tree - Wasnik"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ". Do the followings in HDFS <br>\n",
    ". Remove any folder/files in /tmp that starts with flightData_ <br>\n",
    ". Put the parquet dataset file into /tmp/flightData_in/ <br>\n",
    ". Make sure put was successful (it should have the same size as the lcoal file)! <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/06/08 10:10:45 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "\n",
      "\n",
      "Deleted hdfs://localhost:9000/tmp/flightData_in\n",
      "\n",
      "\n",
      "Found 1 items\n",
      "\n",
      "\n",
      "drwxrwxr-x   - root root          0 2020-05-24 03:38 hdfs://localhost:9000/tmp/flightData_in/flightDelay.parquet\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -chmod -R 777 hdfs://localhost:9000/tmp\n",
    "! hadoop fs -rm    -r  hdfs://localhost:9000/tmp/flightData_*\n",
    "! hadoop fs -mkdir -p  hdfs://localhost:9000/tmp/flightData_in\n",
    "! hadoop fs -put   -p  flightDelay.parquet hdfs://localhost:9000/tmp/flightData_in\n",
    "! hadoop fs -ls        hdfs://localhost:9000/tmp/flightData_in/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdfs://localhost:9000\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!hdfs getconf -confKey fs.defaultFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start a Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://8c79e9d02429:4041\n",
       "SparkContext available as 'sc' (version = 2.4.5, master = local[*], app id = local-1591611063888)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.{SparkSession, DataFrame}\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.sql.types._\n",
       "import org.apache.spark.sql._\n",
       "import org.apache.spark.ml.feature.{VectorAssembler, StringIndexer, VectorIndexer, OneHotEncoder, PCA}\n",
       "import org.apache.spark.mllib.feature.{StandardScaler, StandardScalerModel}\n",
       "import org.apache.spark.mllib.util.MLUtils\n",
       "import org.apache.spark.ml.feature.StandardScaler\n",
       "import org.apache.spark.ml.linalg.{Vector, Vectors}\n",
       "import org.apache.spark.ml.{Pipeline, PipelineStage, PipelineModel}\n",
       "import org.apache.spark.ml.classification.{LogisticRegression, LogisticRegressionModel, RandomForestClassifier, GBTClassifier, DecisionTreeClassifier, DecisionTreeClassificationModel}\n",
       "import org.apache.spark.ml.param.ParamMap\n",
       "import org.apache.s..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Start a simple Spark Session\n",
    "import org.apache.spark.sql.{SparkSession, DataFrame}\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql._\n",
    "\n",
    "//Feature Processing Classes\n",
    "import org.apache.spark.ml.feature.{VectorAssembler,StringIndexer,VectorIndexer,OneHotEncoder, PCA}\n",
    "\n",
    "//standard scaler Classes\n",
    "import org.apache.spark.mllib.feature.{StandardScaler, StandardScalerModel}\n",
    "import org.apache.spark.mllib.util.MLUtils\n",
    "import org.apache.spark.ml.feature.StandardScaler\n",
    "\n",
    "\n",
    "//Linear Algebra Data Structures\n",
    "import org.apache.spark.ml.linalg.{Vector,Vectors}\n",
    "\n",
    "//Model Building Pipeline\n",
    "import org.apache.spark.ml.{Pipeline, PipelineStage, PipelineModel}\n",
    "\n",
    "//Binary Classification\n",
    "import org.apache.spark.ml.classification.{LogisticRegression, LogisticRegressionModel,\n",
    "                                           RandomForestClassifier, GBTClassifier,\n",
    "                                           DecisionTreeClassifier, DecisionTreeClassificationModel}\n",
    "//Model Training\n",
    "import org.apache.spark.ml.param.ParamMap\n",
    "import org.apache.spark.ml.tuning.{CrossValidator, CrossValidatorModel, \n",
    "                                   ParamGridBuilder, TrainValidationSplit}\n",
    "\n",
    "//Model Evaluation\n",
    "import org.apache.spark.ml.evaluation.{BinaryClassificationEvaluator,MulticlassClassificationEvaluator}\n",
    "\n",
    "//Optional: Use the following code below to set the Error reporting\n",
    "import org.apache.log4j._\n",
    "Logger.getLogger(\"org\").setLevel(Level.ERROR)\n",
    "\n",
    "\n",
    "// GBT libraries\n",
    "\n",
    "import org.apache.spark.ml.Pipeline\n",
    "import org.apache.spark.ml.classification.DecisionTreeClassificationModel\n",
    "import org.apache.spark.ml.classification.DecisionTreeClassifier\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "import org.apache.spark.ml.feature.{IndexToString, StringIndexer, VectorIndexer}\n",
    "\n",
    "\n",
    "val spark = SparkSession.builder().appName(\"Group 8 ML Phase 3\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in a parquet file of flight delay, fuel-price and meteorological data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading parque files and tailored dataframes as required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Departing_Port: string (nullable = true)\n",
      " |-- Arriving_Port: string (nullable = true)\n",
      " |-- Airline: string (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month_Num: string (nullable = true)\n",
      " |-- Fuel_Price: double (nullable = true)\n",
      " |-- Departing_Port_station_ID: string (nullable = true)\n",
      " |-- Departing_Port_station_name: string (nullable = true)\n",
      " |-- Arriving_Port_station_ID: string (nullable = true)\n",
      " |-- Arriving_Port_station_name: string (nullable = true)\n",
      " |-- Mean_3pm_cloud_cover_oktas_Depart: double (nullable = true)\n",
      " |-- Mean_3pm_dew_point_temperature_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_3pm_relative_humidity_%_Depart: double (nullable = true)\n",
      " |-- Mean_3pm_temperature_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_3pm_wet_bulb_temperature_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_3pm_wind_speed_km/h_Depart: double (nullable = true)\n",
      " |-- Mean_9am_cloud_cover_okas_Depart: double (nullable = true)\n",
      " |-- Mean_9am_dew_point_temperature_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_9am_relative_humidity_%_Depart: double (nullable = true)\n",
      " |-- Mean_9am_temperature_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_9am_wet_bulb_temperature_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_9am_wind_speed_km/h_Depart: double (nullable = true)\n",
      " |-- Mean_daily_evaporation_mm_Depart: double (nullable = true)\n",
      " |-- Mean_daily_ground_minimum_temperature_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_daily_solar_exposure_MJ/m*m_Depart: double (nullable = true)\n",
      " |-- Mean_daily_sunshine_hours_Depart: double (nullable = true)\n",
      " |-- Mean_daily_wind_run_km_Depart: double (nullable = true)\n",
      " |-- Mean_maximum_temperature_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_minimum_temperature_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_number_of_clear_days_Depart: double (nullable = true)\n",
      " |-- Mean_number_of_cloudy_days_Depart: double (nullable = true)\n",
      " |-- Mean_number_of_days_<_0_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_number_of_days_<_2_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_number_of_days_>_30_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_number_of_days_>_35_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_number_of_days_>_40_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_number_of_days_of_rain_Depart: double (nullable = true)\n",
      " |-- Mean_number_of_days_of_rain_>_1_mm_Depart: double (nullable = true)\n",
      " |-- Mean_number_of_days_of_rain_>_10_mm_Depart: double (nullable = true)\n",
      " |-- Mean_number_of_days_of_rain_>_25_mm_Depart: double (nullable = true)\n",
      " |-- Mean_rainfall_mm_Depart: double (nullable = true)\n",
      " |-- Mean_3pm_cloud_cover_oktas_Arrive: double (nullable = true)\n",
      " |-- Mean_3pm_dew_point_temperature_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_3pm_relative_humidity_%_Arrive: double (nullable = true)\n",
      " |-- Mean_3pm_temperature_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_3pm_wet_bulb_temperature_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_3pm_wind_speed_km/h_Arrive: double (nullable = true)\n",
      " |-- Mean_9am_cloud_cover_okas_Arrive: double (nullable = true)\n",
      " |-- Mean_9am_dew_point_temperature_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_9am_relative_humidity_%_Arrive: double (nullable = true)\n",
      " |-- Mean_9am_temperature_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_9am_wet_bulb_temperature_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_9am_wind_speed_km/h_Arrive: double (nullable = true)\n",
      " |-- Mean_daily_evaporation_mm_Arrive: double (nullable = true)\n",
      " |-- Mean_daily_ground_minimum_temperature_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_daily_solar_exposure_MJ/m*m_Arrive: double (nullable = true)\n",
      " |-- Mean_daily_sunshine_hours_Arrive: double (nullable = true)\n",
      " |-- Mean_daily_wind_run_km_Arrive: double (nullable = true)\n",
      " |-- Mean_maximum_temperature_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_minimum_temperature_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_number_of_clear_days_Arrive: double (nullable = true)\n",
      " |-- Mean_number_of_cloudy_days_Arrive: double (nullable = true)\n",
      " |-- Mean_number_of_days_<_0_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_number_of_days_<_2_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_number_of_days_>_30_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_number_of_days_>_35_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_number_of_days_>_40_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_number_of_days_of_rain_Arrive: double (nullable = true)\n",
      " |-- Mean_number_of_days_of_rain_>_1_mm_Arrive: double (nullable = true)\n",
      " |-- Mean_number_of_days_of_rain_>_10_mm_Arrive: double (nullable = true)\n",
      " |-- Mean_number_of_days_of_rain_>_25_mm_Arrive: double (nullable = true)\n",
      " |-- Mean_rainfall_mm_Arrive: double (nullable = true)\n",
      " |-- Date_Num: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sampleFraction: Double = 0.2\n",
       "flights: org.apache.spark.sql.DataFrame = [Departing_Port: string, Arriving_Port: string ... 72 more fields]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Take a random sample (without replacement) of the data (to reduce memory requirements)\n",
    "val sampleFraction = 0.2\n",
    "\n",
    "val flights = (spark\n",
    "            .read.parquet(\"flightDelay.parquet\")\n",
    "            .withColumn(\"Month_Num1\", $\"Month_Num\" cast \"Int\")\n",
    "            //convert month and year to integer index starting Jan 2004\n",
    "            .withColumn(\"Date_Num\",  ($\"Year\"-2004)*12 + $\"Month_Num1\")\n",
    "            .drop(\"Sectors_Flown\", \"Month_Num1\", \"Change\")\n",
    "            .withColumnRenamed(\"Departures_Delayed\",\"label\")\n",
    "            .withColumnRenamed(\"Price\",\"Fuel_Price\")\n",
    "            //take a sample without replacement\n",
    "            .sample(false,sampleFraction, seed = 222)\n",
    "            //drop NA's even though none were found!\n",
    "            .na.drop()\n",
    "            //.cache\n",
    "              )\n",
    "\n",
    "flights.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing for ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Creating training and test set with down sample the Ontime Departures To Balance The Training and Test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Balancing test set and sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set of the Most Recent 12 Months has 87165 records\n",
      "On time Test Flights: 67957\n",
      "Delayed Test FLights: 19208\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "testing: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Departing_Port: string, Arriving_Port: string ... 72 more fields]\n",
       "ontimeTestFlights: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Departing_Port: string, Arriving_Port: string ... 72 more fields]\n",
       "delayedTestFlights: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Departing_Port: string, Arriving_Port: string ... 72 more fields]\n",
       "test: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Departing_Port: string, Arriving_Port: string ... 72 more fields]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Filter out the most recent 12 months of flight data as the test dataset\n",
    "//Dates after March 2019 have Date_Num > 183\n",
    "val testing = flights.filter($\"Date_Num\"> 183)\n",
    "println(s\"Test Set of the Most Recent 12 Months has ${testing.count()} records\")\n",
    "\n",
    "val ontimeTestFlights = testing.filter($\"label\" === 0)\n",
    "println(s\"On time Test Flights: ${ontimeTestFlights.count()}\")\n",
    "\n",
    "val delayedTestFlights = testing.filter($\"label\" === 1)\n",
    "println(s\"Delayed Test FLights: ${delayedTestFlights.count()}\")\n",
    "\n",
    "//val sampledOntimeTestFlights = ontimeTestFlights.sample(false, 0.2)\n",
    "\n",
    "//val sampleFraction = 0.1\n",
    "\n",
    "val test = (ontimeTestFlights.union(delayedTestFlights))\n",
    "\n",
    "//val sampledCounts = test.groupBy(\"label\").count()\n",
    "//println(\"proportion of lates (label=1) in the sample\")\n",
    "//sampledCounts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Balancing training set and sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On time Training Flights: 976756\n",
      "Delayed Training Flights: 194421\n",
      "Down Sampled ontime Training Flights: 195532\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rawTraining: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Departing_Port: string, Arriving_Port: string ... 72 more fields]\n",
       "ontimeTrainingFlights: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Departing_Port: string, Arriving_Port: string ... 72 more fields]\n",
       "delayedTrainingFlights: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Departing_Port: string, Arriving_Port: string ... 72 more fields]\n",
       "sampledOntimeTrainingFlights: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Departing_Port: string, Arriving_Port: string ... 72 more fields]\n",
       "train: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Departing_Port: string, Arriving_Port: string ... 72 more fields]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Filter out rows prior to the most recent 12 months of flight data as the training dataset\n",
    "val rawTraining = flights.filter($\"Date_Num\" < 184)\n",
    "\n",
    "val ontimeTrainingFlights = rawTraining.filter($\"label\"===0)\n",
    "println(s\"On time Training Flights: ${ontimeTrainingFlights.count()}\")\n",
    "\n",
    "val delayedTrainingFlights = rawTraining.filter($\"label\"===1)\n",
    "println(s\"Delayed Training Flights: ${delayedTrainingFlights.count()}\")\n",
    "\n",
    "val sampledOntimeTrainingFlights = ontimeTrainingFlights.sample(false, 0.2)  \n",
    "\n",
    "println(s\"Down Sampled ontime Training Flights: ${sampledOntimeTrainingFlights.count()}\")\n",
    "\n",
    "//Concatenate rows of ontimeTrainingFlights and delayedTrainingFlights\n",
    "val train = (sampledOntimeTrainingFlights.union(delayedTrainingFlights))\n",
    "               \n",
    "//val resampledCounts = training.groupBy(\"label\").count()\n",
    "//println(\"proportion of lates (label=1) in the sample\")\n",
    "//resampledCounts.show()\n",
    "//training.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Creating Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "getConfusionMatrix: (predictionDF: org.apache.spark.sql.DataFrame)Unit\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def getConfusionMatrix(predictionDF: DataFrame): Unit = {\n",
    "    \n",
    "    println(\"========================Model Assessment Metrics==================================================\\n\")\n",
    "    // Define Binary Classification Evaluator\n",
    "    val binaryEval = new BinaryClassificationEvaluator().setLabelCol(\"label\").setRawPredictionCol(\"rawPrediction\")\n",
    "    // Run Evaluation.  The area under the ROC curve ranges from 0.5 and 1.0 with larger values indicative of better fit\n",
    "    println(s\"Area under ROC: ${binaryEval.setMetricName(\"areaUnderROC\").evaluate(predictionDF)}\")\n",
    "    // Define Multiclass Classification Evaluator\n",
    "    val multiEval = new MulticlassClassificationEvaluator().setLabelCol(\"label\").setPredictionCol(\"prediction\")\n",
    "    println(s\"Accuracy: ${multiEval.setMetricName(\"accuracy\").evaluate(predictionDF)}\")\n",
    "    println(s\"Weighted Precision: ${multiEval.setMetricName(\"weightedPrecision\").evaluate(predictionDF)}\")\n",
    "    println(s\"Weighted Recall: ${multiEval.setMetricName(\"weightedRecall\").evaluate(predictionDF)}\")\n",
    "    println(s\"F1: ${multiEval.setMetricName(\"f1\").evaluate(predictionDF)}\")\n",
    "\n",
    "    val TP = predictionDF.select(\"label\", \"prediction\").filter(\"label = 1 and prediction = 1\").count\n",
    "    val TN = predictionDF.select(\"label\", \"prediction\").filter(\"label = 0 and prediction = 0\").count\n",
    "    val FP = predictionDF.select(\"label\", \"prediction\").filter(\"label = 0 and prediction = 1\").count\n",
    "    val FN = predictionDF.select(\"label\", \"prediction\").filter(\"label = 1 and prediction = 0\").count\n",
    "    val total = predictionDF.select(\"label\").count.toDouble\n",
    "    // Unweighted Metrics\n",
    "    val accuracy    = (TP + TN) / total\n",
    "    val precision   = TP / (TP+FP).toDouble\n",
    "    val recall      = TP / (TP+FN).toDouble\n",
    "    val F1 = 2*precision*recall/(precision+recall)\n",
    "    println(s\"Accuracy: ${accuracy}\")\n",
    "    println(s\"Precision: ${precision}\")\n",
    "    println(s\"Recall: ${recall}\")\n",
    "    println(s\"F1: ${F1}\")\n",
    "\n",
    "    //predictionDF.select( $\"label\",$\"prediction\" cast \"Int\").orderBy(\"label\").groupBy(\"label\").pivot(\"prediction\",Seq(\"0\",\"1\")).count.show()\n",
    "\n",
    "    // Confusion matrix\n",
    "    printf(s\"\"\"|=================== Confusion Matrix ==========================\n",
    "           |##########| %-15s                     %-15s\n",
    "           |----------+----------------------------------------------------\n",
    "           |Actual = 0| %-15d                     %-15d\n",
    "           |Actual = 1| %-15d                     %-15d\n",
    "           |===============================================================\n",
    "         \"\"\".stripMargin, \"Predicted = 0\", \"Predicted = 1\", TN, FP, FN, TP)\n",
    "\n",
    "    println(\"\\n==================================================================================================\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Preparing feature vectors for ML pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pre processing continuos features : Extracting Weather columns along with Date_Num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|    weather_features|\n",
      "+--------------------+\n",
      "|[3.9,7.4,43.0,21....|\n",
      "|[5.8,23.1,69.0,29...|\n",
      "|[5.8,23.1,69.0,29...|\n",
      "|[5.8,23.1,69.0,29...|\n",
      "|[3.9,7.4,43.0,21....|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array: Array[String] = Array(Departing_Port, Arriving_Port, Airline, label, Year, Month_Num, Fuel_Price, Departing_Port_station_ID, Departing_Port_station_name, Arriving_Port_station_ID, Arriving_Port_station_name)\n",
       "columns: Array[String] = Array(Departing_Port, Arriving_Port, Airline, label, Year, Month_Num, Fuel_Price, Departing_Port_station_ID, Departing_Port_station_name, Arriving_Port_station_ID, Arriving_Port_station_name, Mean_3pm_cloud_cover_oktas_Depart, Mean_3pm_dew_point_temperature_Degrees_C_Depart, Mean_3pm_relative_humidity_%_Depart, Mean_3pm_temperature_Degrees_C_Depart, Mean_3pm_wet_bulb_temperature_Degrees_C_Depart, Mean_3pm_wind_speed_km/h_Depart, Mean_9am_cloud_cover_okas_Depart, Mean_9am_dew_point_temperature_Degrees_C_Depart, Mean_9am_relative_humidity_%_Depart, Mean..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// listing columns and extracting as requried\n",
    "val array = Array(\"Departing_Port\", \"Arriving_Port\", \"Airline\", \"label\", \"Year\", \"Month_Num\", \"Fuel_Price\", \n",
    "                  \"Departing_Port_station_ID\", \"Departing_Port_station_name\", \"Arriving_Port_station_ID\", \n",
    "                  \"Arriving_Port_station_name\")\n",
    "val columns = flights.columns\n",
    "\n",
    "val feature_cols = columns.filter(!array.contains(_))\n",
    "\n",
    "\n",
    "// Using VectorAssembler to create single vector feature out of feature_cols\n",
    "\n",
    "val weather_assembler = new VectorAssembler()\n",
    "                        .setInputCols(feature_cols)\n",
    "                        .setOutputCol(\"weather_features\")\n",
    "\n",
    "val weathered_DF = weather_assembler.transform(flights)\n",
    "\n",
    "//weathered_DF.printSchema\n",
    "val weather_features_df = weathered_DF.select(\"weather_features\")\n",
    "weather_features_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: Array[String] = Array(Mean_3pm_cloud_cover_oktas_Depart, Mean_3pm_dew_point_temperature_Degrees_C_Depart, Mean_3pm_relative_humidity_%_Depart, Mean_3pm_temperature_Degrees_C_Depart, Mean_3pm_wet_bulb_temperature_Degrees_C_Depart, Mean_3pm_wind_speed_km/h_Depart, Mean_9am_cloud_cover_okas_Depart, Mean_9am_dew_point_temperature_Degrees_C_Depart, Mean_9am_relative_humidity_%_Depart, Mean_9am_temperature_Degrees_C_Depart, Mean_9am_wet_bulb_temperature_Degrees_C_Depart, Mean_9am_wind_speed_km/h_Depart, Mean_daily_evaporation_mm_Depart, Mean_daily_ground_minimum_temperature_Degrees_C_Depart, Mean_daily_solar_exposure_MJ/m*m_Depart, Mean_daily_sunshine_hours_Depart, Mean_daily_wind_run_km_Depart, Mean_maximum_temperature_Degrees_C_Depart, Mean_minimum_temperature_Degrees_C_Depart, Mean_n..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Standardridzing weather_features_df using StandardScalar such that output feature can be used for PCA to reduce dimensionality of weather features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.StandardScaler\n",
       "std_scalar: org.apache.spark.ml.feature.StandardScaler = stdScal_a421f36905bd\n",
       "pca: org.apache.spark.ml.feature.PCA = pca_51eb3c198604\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.StandardScaler\n",
    "\n",
    "// creating standardarized features\n",
    "val std_scalar = new StandardScaler() \n",
    "                .setInputCol(\"weather_features\")\n",
    "                .setOutputCol(\"standard_features\")\n",
    "                .setWithStd(true)\n",
    "                .setWithMean(false)\n",
    "\n",
    "// implementing to PCA\n",
    "val pca = new PCA()\n",
    "          .setInputCol(\"standard_features\")\n",
    "          .setOutputCol(\"continuos_features\")\n",
    "          .setK(14) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pre-processing Categorical features : usign VectorAssembler to create a single vector feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "categoricalVariables: Array[String] = Array(Departing_Port, Arriving_Port, Airline)\n",
       "categoricalIndexers: Array[org.apache.spark.ml.feature.StringIndexer] = Array(strIdx_23c18795f8ca, strIdx_1512bce97506, strIdx_252fd093f5b3)\n",
       "categoricalEncoders: Array[org.apache.spark.ml.feature.OneHotEncoder] = Array(oneHot_1991c27cd51e, oneHot_96229569f44c, oneHot_2c6aa07560b7)\n",
       "explanatoryFields: Array[String] = Array(Airline_Vec, Fuel_Price, Departing_Port_Vec, Arriving_Port_Vec)\n",
       "categorical_assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_8362291e5029\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Deal with Categorical Columns\n",
    "val categoricalVariables = Array(\n",
    "    \"Departing_Port\", \"Arriving_Port\", \"Airline\")\n",
    "val categoricalIndexers = categoricalVariables\n",
    "  .map(i => new StringIndexer().setInputCol(i).setOutputCol(i+\"_Index\"))\n",
    "val categoricalEncoders = categoricalVariables\n",
    "  .map(e => new OneHotEncoder().setInputCol(e + \"_Index\").setOutputCol(e + \"_Vec\"))\n",
    "\n",
    "\n",
    "// selecting only vectorized explanatory categorical fields\n",
    "val explanatoryFields = Array(\"Airline_Vec\", \"Fuel_Price\",\"Departing_Port_Vec\",\"Arriving_Port_Vec\")\n",
    "\n",
    "// Assemble everything together to be (\"label\",\"features\") format\n",
    "val categorical_assembler = new VectorAssembler()\n",
    "                 .setInputCols(explanatoryFields)\n",
    "                 //.setOutputCol(\"indexedFeatures\")\n",
    "                 .setOutputCol(\"categorical_features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using VectorAssembler again to finally bind categorical and continuous features as features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_0d2732528944\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// creating final features\n",
    "\n",
    "val assembler = new VectorAssembler()\n",
    "                .setInputCols(Array(\"continuos_features\", \"categorical_features\"))\n",
    "                .setOutputCol(\"features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building stages for Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "featureProcessingStages: Array[org.apache.spark.ml.PipelineStage] = Array(vecAssembler_2213e5398914, stdScal_a421f36905bd, pca_51eb3c198604, strIdx_23c18795f8ca, strIdx_1512bce97506, strIdx_252fd093f5b3, oneHot_1991c27cd51e, oneHot_96229569f44c, oneHot_2c6aa07560b7, vecAssembler_8362291e5029, vecAssembler_0d2732528944)\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// establishing stages of pipeling\n",
    "\n",
    "val featureProcessingStages: Array[PipelineStage] = Array(weather_assembler, std_scalar, pca) ++ categoricalIndexers ++ \n",
    "                             categoricalEncoders ++ Array(categorical_assembler) ++ Array(assembler) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.  Estimator Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gbt: org.apache.spark.ml.classification.GBTClassifier = gbtc_4699d751885c\n",
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_139cc9016693\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// creating gradient boosted tree classifier\n",
    "val gbt = new GBTClassifier()\n",
    "         .setLabelCol(\"label\")\n",
    "         .setFeaturesCol(\"features\")\n",
    "         .setMaxIter(10)\n",
    "         .setMaxBins(10)\n",
    "         .setFeatureSubsetStrategy(\"auto\")\n",
    "\n",
    "// assembling a pipeline\n",
    "val pipeline = new Pipeline()\n",
    "              .setStages(featureProcessingStages ++ Array(gbt))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Fitting model into data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model: org.apache.spark.ml.PipelineModel = pipeline_139cc9016693\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// fitting pipeline to training set\n",
    "val model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prediction: org.apache.spark.sql.DataFrame = [Departing_Port: string, Arriving_Port: string ... 86 more fields]\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// transforming test into model\n",
    "val prediction = model.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Model Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================Model Assessment Metrics==================================================\n",
      "\n",
      "Area under ROC: 0.5942017544573059\n",
      "Accuracy: 0.4623874261458154\n",
      "Weighted Precision: 0.7062940886620209\n",
      "Weighted Recall: 0.4623874261458154\n",
      "F1: 0.49474766315920726\n",
      "Accuracy: 0.4623874261458154\n",
      "Precision: 0.2515944736889384\n",
      "Recall: 0.7290712203248646\n",
      "F1: 0.3740934165008214\n",
      "=================== Confusion Matrix ==========================\n",
      "##########| Predicted = 0                       Predicted = 1  \n",
      "----------+----------------------------------------------------\n",
      "Actual = 0| 26300                               41657          \n",
      "Actual = 1| 5204                                14004          \n",
      "===============================================================\n",
      "         \n",
      "==================================================================================================\n"
     ]
    }
   ],
   "source": [
    "// calling a function for model assessment\n",
    "\n",
    "getConfusionMatrix(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
