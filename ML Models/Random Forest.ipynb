{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group 8 Assignment Phase 3 Cross Validated Random Forest Classifiers - Angus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Do the followings in HDFS:\n",
    "\n",
    "* Remove any folder/files in /tmp that starts with flightData_\n",
    "\n",
    "* Create folder /tmp/flightData_in/\n",
    "\n",
    "* Put the parquet dataset file into /tmp/flightData_in/\n",
    "\n",
    "* Make sure put was successfull (it should have the same size as the local file)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/06/08 10:28:37 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "\n",
      "\n",
      "Deleted hdfs://localhost:9000/tmp/flightData_in\n",
      "\n",
      "\n",
      "Found 1 items\n",
      "\n",
      "\n",
      "drwxrwxr-x   - root root          0 2020-05-24 03:38 hdfs://localhost:9000/tmp/flightData_in/flightDelay.parquet\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -chmod -R 777 hdfs://localhost:9000/tmp\n",
    "! hadoop fs -rm    -r  hdfs://localhost:9000/tmp/flightData_*\n",
    "! hadoop fs -mkdir -p  hdfs://localhost:9000/tmp/flightData_in\n",
    "! hadoop fs -put   -p  flightDelay.parquet hdfs://localhost:9000/tmp/flightData_in\n",
    "! hadoop fs -ls        hdfs://localhost:9000/tmp/flightData_in/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdfs://localhost:9000\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!hdfs getconf -confKey fs.defaultFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://8c79e9d02429:4043\n",
       "SparkContext available as 'sc' (version = 2.4.5, master = local[*], app id = local-1591612134983)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.{SparkSession, DataFrame}\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.sql.types._\n",
       "import org.apache.spark.sql._\n",
       "import org.apache.spark.ml.attribute._\n",
       "import org.apache.spark.ml.feature.{VectorAssembler, StringIndexer, VectorIndexer, OneHotEncoder, PCA, Normalizer}\n",
       "import org.apache.spark.ml.linalg.{Vector, Vectors}\n",
       "import org.apache.spark.ml.{Pipeline, PipelineStage, PipelineModel}\n",
       "import org.apache.spark.ml.classification.{LogisticRegression, LogisticRegressionModel, BinaryLogisticRegressionSummary, RandomForestClassifier, GBTClassifier, DecisionTreeClassifier, DecisionTreeClassificationModel, MultilayerPerceptronClassifier}\n",
       "import org.apache.spark.ml.param.ParamMap\n",
       "import org.apache.spark.ml.tuning.{CrossValidator, CrossValidatorModel, ..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Start a simple Spark Session\n",
    "import org.apache.spark.sql.{SparkSession, DataFrame}\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.ml.attribute._\n",
    "\n",
    "//Feature pre-Processing Classes\n",
    "import org.apache.spark.ml.feature.{VectorAssembler,StringIndexer,\n",
    "                                    VectorIndexer,OneHotEncoder, PCA, Normalizer}\n",
    "\n",
    "//Linear Algebra Data Structures\n",
    "import org.apache.spark.ml.linalg.{Vector,Vectors}\n",
    "\n",
    "//Model Building Pipeline\n",
    "import org.apache.spark.ml.{Pipeline, PipelineStage, PipelineModel}\n",
    "\n",
    "//Binary Classification\n",
    "import org.apache.spark.ml.classification.{LogisticRegression, LogisticRegressionModel, BinaryLogisticRegressionSummary,\n",
    "                                           RandomForestClassifier, GBTClassifier,\n",
    "                                           DecisionTreeClassifier, DecisionTreeClassificationModel, MultilayerPerceptronClassifier}\n",
    "//Model Training\n",
    "import org.apache.spark.ml.param.ParamMap\n",
    "import org.apache.spark.ml.tuning.{CrossValidator, CrossValidatorModel, \n",
    "                                   ParamGridBuilder, TrainValidationSplit}\n",
    "\n",
    "//Model Evaluation\n",
    "import org.apache.spark.ml.evaluation.{BinaryClassificationEvaluator,MulticlassClassificationEvaluator}\n",
    "\n",
    "//Optional: Use the following code below to set the Error reporting\n",
    "import org.apache.log4j._\n",
    "Logger.getLogger(\"org\").setLevel(Level.ERROR)\n",
    "\n",
    "\n",
    "//For Cleaning\n",
    "//import scala.util.matching.Regex\n",
    "\n",
    "val spark = SparkSession.builder().appName(\"Group 8 ML Phase 3\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in a parquet file of flight delay, fuel-price and meteorological data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Departing_Port: string (nullable = true)\n",
      " |-- Arriving_Port: string (nullable = true)\n",
      " |-- Airline: string (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month_Num: string (nullable = true)\n",
      " |-- Fuel_Price: double (nullable = true)\n",
      " |-- Departing_Port_station_ID: string (nullable = true)\n",
      " |-- Departing_Port_station_name: string (nullable = true)\n",
      " |-- Arriving_Port_station_ID: string (nullable = true)\n",
      " |-- Arriving_Port_station_name: string (nullable = true)\n",
      " |-- Mean_3pm_cloud_cover_oktas_Depart: double (nullable = true)\n",
      " |-- Mean_3pm_dew_point_temperature_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_3pm_relative_humidity_%_Depart: double (nullable = true)\n",
      " |-- Mean_3pm_temperature_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_3pm_wet_bulb_temperature_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_3pm_wind_speed_km/h_Depart: double (nullable = true)\n",
      " |-- Mean_9am_cloud_cover_okas_Depart: double (nullable = true)\n",
      " |-- Mean_9am_dew_point_temperature_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_9am_relative_humidity_%_Depart: double (nullable = true)\n",
      " |-- Mean_9am_temperature_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_9am_wet_bulb_temperature_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_9am_wind_speed_km/h_Depart: double (nullable = true)\n",
      " |-- Mean_daily_evaporation_mm_Depart: double (nullable = true)\n",
      " |-- Mean_daily_ground_minimum_temperature_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_daily_solar_exposure_MJ/m*m_Depart: double (nullable = true)\n",
      " |-- Mean_daily_sunshine_hours_Depart: double (nullable = true)\n",
      " |-- Mean_daily_wind_run_km_Depart: double (nullable = true)\n",
      " |-- Mean_maximum_temperature_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_minimum_temperature_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_number_of_clear_days_Depart: double (nullable = true)\n",
      " |-- Mean_number_of_cloudy_days_Depart: double (nullable = true)\n",
      " |-- Mean_number_of_days_<_0_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_number_of_days_<_2_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_number_of_days_>_30_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_number_of_days_>_35_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_number_of_days_>_40_Degrees_C_Depart: double (nullable = true)\n",
      " |-- Mean_number_of_days_of_rain_Depart: double (nullable = true)\n",
      " |-- Mean_number_of_days_of_rain_>_1_mm_Depart: double (nullable = true)\n",
      " |-- Mean_number_of_days_of_rain_>_10_mm_Depart: double (nullable = true)\n",
      " |-- Mean_number_of_days_of_rain_>_25_mm_Depart: double (nullable = true)\n",
      " |-- Mean_rainfall_mm_Depart: double (nullable = true)\n",
      " |-- Mean_3pm_cloud_cover_oktas_Arrive: double (nullable = true)\n",
      " |-- Mean_3pm_dew_point_temperature_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_3pm_relative_humidity_%_Arrive: double (nullable = true)\n",
      " |-- Mean_3pm_temperature_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_3pm_wet_bulb_temperature_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_3pm_wind_speed_km/h_Arrive: double (nullable = true)\n",
      " |-- Mean_9am_cloud_cover_okas_Arrive: double (nullable = true)\n",
      " |-- Mean_9am_dew_point_temperature_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_9am_relative_humidity_%_Arrive: double (nullable = true)\n",
      " |-- Mean_9am_temperature_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_9am_wet_bulb_temperature_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_9am_wind_speed_km/h_Arrive: double (nullable = true)\n",
      " |-- Mean_daily_evaporation_mm_Arrive: double (nullable = true)\n",
      " |-- Mean_daily_ground_minimum_temperature_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_daily_solar_exposure_MJ/m*m_Arrive: double (nullable = true)\n",
      " |-- Mean_daily_sunshine_hours_Arrive: double (nullable = true)\n",
      " |-- Mean_daily_wind_run_km_Arrive: double (nullable = true)\n",
      " |-- Mean_maximum_temperature_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_minimum_temperature_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_number_of_clear_days_Arrive: double (nullable = true)\n",
      " |-- Mean_number_of_cloudy_days_Arrive: double (nullable = true)\n",
      " |-- Mean_number_of_days_<_0_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_number_of_days_<_2_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_number_of_days_>_30_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_number_of_days_>_35_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_number_of_days_>_40_Degrees_C_Arrive: double (nullable = true)\n",
      " |-- Mean_number_of_days_of_rain_Arrive: double (nullable = true)\n",
      " |-- Mean_number_of_days_of_rain_>_1_mm_Arrive: double (nullable = true)\n",
      " |-- Mean_number_of_days_of_rain_>_10_mm_Arrive: double (nullable = true)\n",
      " |-- Mean_number_of_days_of_rain_>_25_mm_Arrive: double (nullable = true)\n",
      " |-- Mean_rainfall_mm_Arrive: double (nullable = true)\n",
      " |-- Date_Num: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "flights: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Departing_Port: string, Arriving_Port: string ... 72 more fields]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val flights = (spark\n",
    "            .read.parquet(\"flightDelay.parquet\")\n",
    "            //.read.parquet(\"hdfs://localhost:9000/tmp/flightData_in/flightDelay.parquet\") //uncomment to load from hdfs\n",
    "            .withColumn(\"Month_Num1\", $\"Month_Num\" cast \"Int\")\n",
    "            //convert month and year to integer index starting Jan 2004\n",
    "            .withColumn(\"Date_Num\",  ($\"Year\"-2004)*12 + $\"Month_Num1\")\n",
    "            .drop(\"Sectors_Flown\", \"Month_Num1\", \"Change\")\n",
    "            .withColumnRenamed(\"Departures_Delayed\",\"label\")\n",
    "            .withColumnRenamed(\"Price\",\"Fuel_Price\")\n",
    "            //drop NA's even though none were found!\n",
    "            .na.drop()\n",
    "            .sample(false, 0.2) //delete this in production mode\n",
    "            //.cache\n",
    "              )\n",
    "\n",
    "flights.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split The Data into training and testing dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set of the Most Recent 12 Months has 86981 records\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "testing: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Departing_Port: string, Arriving_Port: string ... 72 more fields]\n",
       "rawTraining: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Departing_Port: string, Arriving_Port: string ... 72 more fields]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Filter out the most recent 12 months of flight data as the test dataset\n",
    "val testing = flights.filter($\"Date_Num\"> 183).cache()\n",
    "println(s\"Test Set of the Most Recent 12 Months has ${testing.count()} records\")\n",
    "\n",
    "//Filter out rows prior to the most recent 12 months of flight data as the training dataset\n",
    "val rawTraining = flights.filter($\"Date_Num\" < 184)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Down sample the Ontime Departures To Balance The Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On time Training Flights: 978002\n",
      "Delayed Training Flights: 195310\n",
      "Down Sampled ontime Training Flights: 195219\n",
      "proportion of lates (label=1) in the sample\n",
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    1|19378|\n",
      "|    0|19522|\n",
      "+-----+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ontimeTrainingFlights: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Departing_Port: string, Arriving_Port: string ... 72 more fields]\n",
       "delayedTrainingFlights: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Departing_Port: string, Arriving_Port: string ... 72 more fields]\n",
       "downSampleFraction: Double = 0.2\n",
       "sampledOntimeTrainingFlights: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Departing_Port: string, Arriving_Port: string ... 72 more fields]\n",
       "localTestingSampleFraction: Double = 0.1\n",
       "training: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Departing_Port: string, Arriving_Port: string ... 72 more fields]\n",
       "resampledCounts: org.apache.spark.sql.DataFrame = [label: int, count: bigint]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ontimeTrainingFlights = rawTraining.filter($\"label\"===0)\n",
    "println(s\"On time Training Flights: ${ontimeTrainingFlights.count()}\")\n",
    "\n",
    "val delayedTrainingFlights = rawTraining.filter($\"label\"===1)\n",
    "println(s\"Delayed Training Flights: ${delayedTrainingFlights.count()}\")\n",
    "\n",
    "//ontime:delayed approx 5:1 so take a random sample of size fifth of the ontime departures\n",
    "val downSampleFraction = 0.2\n",
    "val sampledOntimeTrainingFlights = ontimeTrainingFlights.sample(false, downSampleFraction)  \n",
    "\n",
    "println(s\"Down Sampled ontime Training Flights: ${sampledOntimeTrainingFlights.count()}\")\n",
    "\n",
    "//down sample resulting training set for the purposes of local testing\n",
    "val localTestingSampleFraction = 0.1\n",
    "//Concatenate rows of ontimeTrainingFlights and delayedTrainingFlights\n",
    "val training = (sampledOntimeTrainingFlights\n",
    "                .union(delayedTrainingFlights)\n",
    "                .sample(false, localTestingSampleFraction)\n",
    "                .cache())\n",
    "               \n",
    "val resampledCounts = training.groupBy(\"label\").count()\n",
    "println(\"proportion of lates (label=1) in the sample\")\n",
    "resampledCounts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Assessment via a Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "getConfusionMatrix: (predictionDF: org.apache.spark.sql.DataFrame)Unit\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getConfusionMatrix(predictionDF: DataFrame): Unit = {\n",
    "    \n",
    "    println(\"========================Model Assessment Metrics==================================================\\n\")\n",
    "    // Define Binary Classification Evaluator\n",
    "    val binaryEval = new BinaryClassificationEvaluator().setLabelCol(\"label\").setRawPredictionCol(\"rawPrediction\")\n",
    "    // Run Evaluation.  The area under the ROC curve ranges from 0.5 and 1.0 with larger values indicative of better fit\n",
    "    println(s\"Area under ROC: ${binaryEval.setMetricName(\"areaUnderROC\").evaluate(predictionDF)}\")\n",
    "    // Define Multiclass Classification Evaluator\n",
    "    val multiEval = new MulticlassClassificationEvaluator().setLabelCol(\"label\").setPredictionCol(\"prediction\")\n",
    "    println(s\"Accuracy: ${multiEval.setMetricName(\"accuracy\").evaluate(predictionDF)}\")\n",
    "    println(s\"Weighted Precision: ${multiEval.setMetricName(\"weightedPrecision\").evaluate(predictionDF)}\")\n",
    "    println(s\"Weighted Recall: ${multiEval.setMetricName(\"weightedRecall\").evaluate(predictionDF)}\")\n",
    "    println(s\"F1: ${multiEval.setMetricName(\"f1\").evaluate(predictionDF)}\")\n",
    "\n",
    "    val TP = predictionDF.select(\"label\", \"prediction\").filter(\"label = 1 and prediction = 1\").count\n",
    "    val TN = predictionDF.select(\"label\", \"prediction\").filter(\"label = 0 and prediction = 0\").count\n",
    "    val FP = predictionDF.select(\"label\", \"prediction\").filter(\"label = 0 and prediction = 1\").count\n",
    "    val FN = predictionDF.select(\"label\", \"prediction\").filter(\"label = 1 and prediction = 0\").count\n",
    "    val total = predictionDF.select(\"label\").count.toDouble\n",
    "    // Unweighted Metrics\n",
    "    val accuracy    = (TP + TN) / total\n",
    "    val precision   = TP / (TP+FP).toDouble\n",
    "    val recall      = TP / (TP+FN).toDouble\n",
    "    val F1 = 2*precision*recall/(precision+recall)\n",
    "    println(s\"Accuracy: ${accuracy}\")\n",
    "    println(s\"Precision: ${precision}\")\n",
    "    println(s\"Recall: ${recall}\")\n",
    "    println(s\"F1: ${F1}\")\n",
    "\n",
    "    //predictionDF.select( $\"label\",$\"prediction\" cast \"Int\").orderBy(\"label\").groupBy(\"label\").pivot(\"prediction\",Seq(\"0\",\"1\")).count.show()\n",
    "\n",
    "    // Confusion matrix\n",
    "    printf(s\"\"\"|=================== Confusion Matrix ==========================\n",
    "           |##########| %-15s                     %-15s\n",
    "           |----------+----------------------------------------------------\n",
    "           |Actual = 0| %-15d                     %-15d\n",
    "           |Actual = 1| %-15d                     %-15d\n",
    "           |===============================================================\n",
    "         \"\"\".stripMargin, \"Predicted = 0\", \"Predicted = 1\", TN, FP, FN, TP)\n",
    "\n",
    "    println(\"\\n==================================================================================================\")\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Flight Data Feature Processing Pipleline Stages for Arbitrary Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "categoricalVariables: Array[String] = Array(Departing_Port, Arriving_Port, Airline)\n",
       "categoricalIndexers: Array[org.apache.spark.ml.feature.StringIndexer] = Array(strIdx_66e3401730d4, strIdx_207284171ed0, strIdx_78feabb4d3e7)\n",
       "categoricalEncoders: Array[org.apache.spark.ml.feature.OneHotEncoder] = Array(oneHot_9251aefffc36, oneHot_3d93e216ed5d, oneHot_e799ad96be47)\n",
       "explanatoryFields: Array[String] = Array(Date_Num, Airline_Vec, Fuel_Price, Departing_Port_Vec, Mean_daily_wind_run_km_Depart, Mean_rainfall_mm_Depart, Mean_number_of_days_of_rain_Depart, Mean_number_of_days_>_40_Degrees_C_Depart, Arriving_Port_Vec)\n",
       "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_5d9387ee0e7b\n",
       "featureProcessingStages: Array[org.apache.spark.ml.PipelineStage] = Array(strIdx_66e3401730d4, str..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Deal with Categorical Columns\n",
    "val categoricalVariables = Array(\n",
    "    \"Departing_Port\", \"Arriving_Port\", \"Airline\")\n",
    "val categoricalIndexers = categoricalVariables\n",
    "  .map(i => new StringIndexer().setInputCol(i).setOutputCol(i+\"_Index\"))\n",
    "val categoricalEncoders = categoricalVariables\n",
    "  .map(e => new OneHotEncoder().setInputCol(e + \"_Index\").setOutputCol(e + \"_Vec\"))\n",
    "\n",
    "\n",
    "// select the flight data explanatory fields that will predict flight delay\n",
    "val explanatoryFields = Array(\"Date_Num\",  \"Airline_Vec\", \"Fuel_Price\",\n",
    "    \"Departing_Port_Vec\", \"Mean_daily_wind_run_km_Depart\", \"Mean_rainfall_mm_Depart\",\n",
    "    \"Mean_number_of_days_of_rain_Depart\",\"Mean_number_of_days_>_40_Degrees_C_Depart\",\n",
    "    \"Arriving_Port_Vec\")\n",
    "\n",
    "// Assemble everything together to be (\"label\",\"features\") format\n",
    "val assembler = (new VectorAssembler()\n",
    "                 .setInputCols(explanatoryFields)\n",
    "                 //.setOutputCol(\"indexedFeatures\")\n",
    "                 .setOutputCol(\"features\")\n",
    "                )\n",
    "\n",
    "val featureProcessingStages: Array[PipelineStage] = categoricalIndexers ++ categoricalEncoders ++ Array(assembler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pipeLineCrossValidationTrainer: (pipeLine: org.apache.spark.ml.Pipeline, paramGrid: Array[org.apache.spark.ml.param.ParamMap], training: org.apache.spark.sql.DataFrame, testing: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pipeLineCrossValidationTrainer(pipeLine: Pipeline, paramGrid: Array[ParamMap], training: DataFrame, testing: DataFrame): DataFrame = {\n",
    "\n",
    "    //Use Cross Validation to Train the Decision Tree Models\n",
    "    val cv = new CrossValidator()\n",
    "      .setEstimator(pipeLine)\n",
    "      .setEvaluator(new BinaryClassificationEvaluator)\n",
    "      .setEstimatorParamMaps(paramGrid)\n",
    "      .setNumFolds(10)  // Use 3+ in practice\n",
    "      //.setParallelism(2)  // Evaluate up to 2 parameter settings in parallel\n",
    "\n",
    "\n",
    "    // Run cross-validation, and choose the best set of parameters.\n",
    "    val cvModel = cv.fit(training)\n",
    "\n",
    "    // test the model\n",
    "    cvModel.transform(testing)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validated RF Classifier Prototype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest parameters:\n",
      " cacheNodeIds: If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. (default: false)\n",
      "checkpointInterval: set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext (default: 10)\n",
      "featureSubsetStrategy: The number of features to consider for splits at each tree node. Supported options: auto, all, onethird, sqrt, log2, (0.0-1.0], [1-n]. (default: auto, current: auto)\n",
      "featuresCol: features column name (default: features, current: features)\n",
      "impurity: Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini (default: gini, current: gini)\n",
      "labelCol: label column name (default: label, current: label)\n",
      "maxBins: Max number of bins for discretizing continuous features.  Must be at least 2 and at least number of categories for any categorical feature. (default: 32)\n",
      "maxDepth: Maximum depth of the tree. (Nonnegative) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. (default: 5)\n",
      "maxMemoryInMB: Maximum memory in MB allocated to histogram aggregation. (default: 256, current: 4096)\n",
      "minInfoGain: Minimum information gain for a split to be considered at a tree node. (default: 0.0)\n",
      "minInstancesPerNode: Minimum number of instances each child must have after split.  If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Must be at least 1. (default: 1)\n",
      "numTrees: Number of trees to train (at least 1) (default: 20)\n",
      "predictionCol: prediction column name (default: prediction)\n",
      "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities (default: probability)\n",
      "rawPredictionCol: raw prediction (a.k.a. confidence) column name (default: rawPrediction)\n",
      "seed: random seed (default: 207336481)\n",
      "subsamplingRate: Fraction of the training data used for learning each decision tree, in range (0, 1]. (default: 1.0)\n",
      "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0 excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold (undefined)\n",
      "\n",
      "+----------+-----+--------------------+\n",
      "|prediction|label|            features|\n",
      "+----------+-----+--------------------+\n",
      "|       1.0|    1|(101,[0,4,10,30,5...|\n",
      "|       1.0|    1|(101,[0,4,10,30,5...|\n",
      "|       1.0|    1|(101,[0,4,10,30,5...|\n",
      "|       1.0|    1|(101,[0,6,10,30,5...|\n",
      "|       1.0|    1|(101,[0,6,10,30,5...|\n",
      "|       1.0|    1|(101,[0,6,10,30,5...|\n",
      "|       1.0|    1|(101,[0,6,10,30,5...|\n",
      "|       1.0|    1|(101,[0,4,10,30,5...|\n",
      "|       1.0|    1|(101,[0,4,10,30,5...|\n",
      "|       1.0|    1|(101,[0,6,10,30,5...|\n",
      "|       1.0|    1|(101,[0,6,10,30,5...|\n",
      "|       1.0|    1|(101,[0,6,10,30,5...|\n",
      "|       1.0|    1|(101,[0,4,10,30,5...|\n",
      "|       1.0|    1|(101,[0,4,10,30,5...|\n",
      "|       1.0|    1|(101,[0,4,10,30,5...|\n",
      "|       1.0|    1|(101,[0,4,10,30,5...|\n",
      "|       1.0|    1|(101,[0,4,10,30,5...|\n",
      "|       1.0|    1|(101,[0,4,10,30,5...|\n",
      "|       1.0|    1|(101,[0,6,10,30,5...|\n",
      "|       1.0|    1|(101,[0,6,10,30,5...|\n",
      "+----------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "========================Model Assessment Metrics==================================================\n",
      "\n",
      "Area under ROC: 0.5866336865803233\n",
      "Accuracy: 0.45632954323357977\n",
      "Weighted Precision: 0.7029393901785709\n",
      "Weighted Recall: 0.4563295432335797\n",
      "F1: 0.4877708602053547\n",
      "Accuracy: 0.45632954323357977\n",
      "Precision: 0.24989751724383766\n",
      "Recall: 0.7293487307532251\n",
      "F1: 0.3722504679348475\n",
      "=================== Confusion Matrix ==========================\n",
      "##########| Predicted = 0                       Predicted = 1  \n",
      "----------+----------------------------------------------------\n",
      "Actual = 0| 25671                               42086          \n",
      "Actual = 1| 5203                                14021          \n",
      "===============================================================\n",
      "         \n",
      "==================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rf: org.apache.spark.ml.classification.RandomForestClassifier = rfc_1594db7ecb41\n",
       "rfPipeline: org.apache.spark.ml.Pipeline = pipeline_2b54d8786791\n",
       "rfParamGrid: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\trfc_1594db7ecb41-maxBins: 32,\n",
       "\trfc_1594db7ecb41-maxDepth: 3\n",
       "}, {\n",
       "\trfc_1594db7ecb41-maxBins: 32,\n",
       "\trfc_1594db7ecb41-maxDepth: 6\n",
       "})\n",
       "rfCVPredictions: org.apache.spark.sql.DataFrame = [Departing_Port: string, Arriving_Port: string ... 82 more fields]\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// instantiate the random forest classifier\n",
    "val rf = new RandomForestClassifier()\n",
    "  .setLabelCol(\"label\")\n",
    "  .setFeaturesCol(\"features\")\n",
    "  .setMaxMemoryInMB(4096)\n",
    "  .setImpurity(\"gini\")\n",
    "  .setFeatureSubsetStrategy(\"auto\")\n",
    "\n",
    "println(s\"Random Forest parameters:\\n ${rf.explainParams()}\\n\")\n",
    "\n",
    "// build the random forest pipeline\n",
    "val rfPipeline = new Pipeline().setStages(featureProcessingStages ++ Array(rf))\n",
    "\n",
    "\n",
    "val rfParamGrid = new ParamGridBuilder()\n",
    "             .addGrid(rf.maxDepth, Array(3,6))\n",
    "             .addGrid(rf.maxBins, Array(32))\n",
    "             .build()\n",
    "\n",
    "// train and test random forest classifier\n",
    "val rfCVPredictions = pipeLineCrossValidationTrainer(rfPipeline, rfParamGrid, training, testing)\n",
    "\n",
    "rfCVPredictions.select(\"prediction\", \"label\", \"features\").show(20)\n",
    "getConfusionMatrix(rfCVPredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
