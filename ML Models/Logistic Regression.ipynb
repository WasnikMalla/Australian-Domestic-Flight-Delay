{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group 8 Assignment Phase 3 Logistic Regression with PCA for Weather and CV (Tod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Do the followings in HDFS:\n",
    "\n",
    "* Remove any folder/files in /tmp that starts with flightData_,\n",
    "\n",
    "* Create folder /tmp/flightData_in/,\n",
    "\n",
    "* Put the parquet dataset file into /tmp/flightData_in/,\n",
    "\n",
    "* Make sure put was successfull (it should have the same size as the local file)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `hdfs://localhost:9000/tmp/flightData_*': No such file or directory\n",
      "\n",
      "\n",
      "Found 1 items\n",
      "\n",
      "\n",
      "drwxrwxr-x   - root root          0 2020-05-24 03:38 hdfs://localhost:9000/tmp/flightData_in/flightDelay.parquet\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -chmod -R 777 hdfs://localhost:9000/tmp\n",
    "! hadoop fs -rm    -r  hdfs://localhost:9000/tmp/flightData_*\n",
    "! hadoop fs -mkdir -p  hdfs://localhost:9000/tmp/flightData_in\n",
    "! hadoop fs -put   -p  flightDelay.parquet hdfs://localhost:9000/tmp/flightData_in\n",
    "! hadoop fs -ls        hdfs://localhost:9000/tmp/flightData_in/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdfs://localhost:9000\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!hdfs getconf -confKey fs.defaultFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Requisite Libraries and Start a Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://8c79e9d02429:4040\n",
       "SparkContext available as 'sc' (version = 2.4.5, master = local[*], app id = local-1591610792470)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.expressions.Window\n",
       "import org.apache.spark.sql.{SparkSession, DataFrame}\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.sql.types._\n",
       "import org.apache.spark.sql._\n",
       "import org.apache.spark.ml.attribute._\n",
       "import org.apache.spark.ml.feature.{VectorAssembler, StringIndexer, StandardScaler, VectorIndexer, OneHotEncoder, PCA, Normalizer}\n",
       "import org.apache.spark.ml.linalg.{Vector, Vectors}\n",
       "import org.apache.spark.ml.{Pipeline, PipelineStage, PipelineModel}\n",
       "import org.apache.spark.ml.classification.{LogisticRegression, LogisticRegressionModel, BinaryLogisticRegressionSummary}\n",
       "import org.apache.spark.ml.param.ParamMap\n",
       "import org.apache.spark.ml.tuning.{CrossValidator, CrossValidatorModel, ParamGridBuilder}\n",
       "import org.apache.spark.ml.evaluation.{BinaryCl..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Start a simple Spark Session\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "import org.apache.spark.sql.{SparkSession, DataFrame}\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql._\n",
    "\n",
    "import org.apache.spark.ml.attribute._\n",
    "\n",
    "//Feature pre-Processing Classes\n",
    "import org.apache.spark.ml.feature.{VectorAssembler,StringIndexer,StandardScaler,\n",
    "                                    VectorIndexer,OneHotEncoder, PCA, Normalizer}\n",
    "\n",
    "\n",
    "//Linear Algebra Data Structures\n",
    "import org.apache.spark.ml.linalg.{Vector,Vectors}\n",
    "\n",
    "//Model Building Pipeline\n",
    "import org.apache.spark.ml.{Pipeline, PipelineStage, PipelineModel}\n",
    "\n",
    "//Binary Classification\n",
    "import org.apache.spark.ml.classification.{LogisticRegression, LogisticRegressionModel, BinaryLogisticRegressionSummary}\n",
    "//Model Training\n",
    "import org.apache.spark.ml.param.ParamMap\n",
    "import org.apache.spark.ml.tuning.{CrossValidator, CrossValidatorModel, ParamGridBuilder}\n",
    "\n",
    "//Model Evaluation\n",
    "import org.apache.spark.ml.evaluation.{BinaryClassificationEvaluator,MulticlassClassificationEvaluator}\n",
    "\n",
    "//Optional: Use the following code below to set the Error reporting\n",
    "import org.apache.log4j._\n",
    "Logger.getLogger(\"org\").setLevel(Level.ERROR)\n",
    "\n",
    "val spark = SparkSession.builder().appName(\"Group 8 ML Phase 3\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in a parquet file of flight delay, fuel-price and meteorological data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rawFlights: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Departing_Port: string, Arriving_Port: string ... 72 more fields]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rawFlights = (spark\n",
    "            .read.parquet(\"flightDelay.parquet\") //Delete in production mode\n",
    "            //.read.parquet(\"hdfs://localhost:9000/tmp/flightData_in/flightDelay.parquet\")\n",
    "            .withColumn(\"Month_Num1\", $\"Month_Num\" cast \"Int\")\n",
    "            //convert month and year to integer index starting Jan 2004\n",
    "            .withColumn(\"Date_Num\",  ($\"Year\"-2004)*12 + $\"Month_Num1\")\n",
    "            .drop(\"Sectors_Flown\", \"Month_Num1\", \"Change\")\n",
    "            .withColumnRenamed(\"Departures_Delayed\",\"label\")\n",
    "            .withColumnRenamed(\"Price\",\"Fuel_Price\")\n",
    "            .sample(false, 0.2) //delete this in production mode\n",
    "            //.na.drop()\n",
    "            //.cache\n",
    "              )\n",
    "\n",
    "//rawFlights.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## index rows by volume of traffic, routeID = 1, has the least traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+-------------+----------------+-------+----------+----------+-----------------------------+-----------------------+----------------------------------+-----------------------------------------+\n",
      "|Date_Num|Departing_Port|Arriving_Port|Airline         |routeID|numFlights|Fuel_Price|Mean_daily_wind_run_km_Depart|Mean_rainfall_mm_Depart|Mean_number_of_days_of_rain_Depart|Mean_number_of_days_>_40_Degrees_C_Depart|\n",
      "+--------+--------------+-------------+----------------+-------+----------+----------+-----------------------------+-----------------------+----------------------------------+-----------------------------------------+\n",
      "|1       |Cairns        |Sydney       |Qantas          |114    |10109     |1.3       |323.0                        |399.6                  |18.5                              |0.0                                      |\n",
      "|64      |Wagga Wagga   |Sydney       |Regional Express|103    |9073      |1.92      |241.0                        |40.4                   |6.7                               |0.0                                      |\n",
      "|1       |Cairns        |Sydney       |Qantas          |114    |10109     |1.3       |323.0                        |399.6                  |18.5                              |0.0                                      |\n",
      "|2       |Cairns        |Sydney       |Qantas          |114    |10109     |1.2       |314.0                        |442.9                  |18.7                              |0.0                                      |\n",
      "|64      |Wagga Wagga   |Sydney       |Regional Express|103    |9073      |1.92      |241.0                        |40.4                   |6.7                               |0.0                                      |\n",
      "+--------+--------------+-------------+----------------+-------+----------+----------+-----------------------------+-----------------------+----------------------------------+-----------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "routes: org.apache.spark.sql.DataFrame = [Departing_Port: string, Arriving_Port: string ... 2 more fields]\n",
       "flights: org.apache.spark.sql.DataFrame = [Airline: string, label: int ... 74 more fields]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val routes = rawFlights\n",
    "    .groupBy(\"Departing_Port\",\"Arriving_Port\")\n",
    "    .agg(expr(\"count(*) as numFlights\"))\n",
    "    .orderBy(desc(\"numFlights\"))\n",
    "    .withColumn(\"routeID\",row_number().over(Window.orderBy(\"numFlights\")))\n",
    "\n",
    "val flights = rawFlights.join(\n",
    "    routes, \n",
    "    rawFlights((\"Departing_Port\")) <=> routes((\"Departing_Port\"))\n",
    "        && rawFlights((\"Arriving_Port\")) <=> routes((\"Arriving_Port\")),\n",
    "    \"left\"\n",
    ").drop(rawFlights.col(\"Departing_Port\")).drop(rawFlights.col(\"Arriving_Port\"))\n",
    "\n",
    "\n",
    "//flights.show(2,false)\n",
    "\n",
    "flights.select(\"Date_Num\",  \"Departing_Port\", \"Arriving_Port\", \"Airline\", \"routeID\", \"numFlights\", \"Fuel_Price\",\"Mean_daily_wind_run_km_Depart\", \"Mean_rainfall_mm_Depart\",\n",
    "    \"Mean_number_of_days_of_rain_Depart\",\"Mean_number_of_days_>_40_Degrees_C_Depart\").show(5,false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data into training and testing dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set of the Most Recent 12 Months has 87268 records\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "testing: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Airline: string, label: int ... 74 more fields]\n",
       "rawTraining: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Airline: string, label: int ... 74 more fields]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Filter out the most recent 12 months of flight data as the test dataset\n",
    "//Take a sample of Dates after March 2019 have Date_Num > 183 for the testing data\n",
    "val testing = flights.filter($\"Date_Num\"> 183).cache()\n",
    "println(s\"Test Set of the Most Recent 12 Months has ${testing.count()} records\")\n",
    "\n",
    "//Filter out rows prior to the most recent 12 months of flight data as the training dataset\n",
    "val rawTraining = flights.filter($\"Date_Num\" < 184)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Down sample the Ontime Departures To Balance The Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On time Training Flights: 975678\n",
      "Delayed Training Flights: 195349\n",
      "Down Sampled ontime Training Flights: 195630\n",
      "proportion of lates (label=1) in the sample\n",
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    1|19464|\n",
      "|    0|19381|\n",
      "+-----+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ontimeTrainingFlights: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Airline: string, label: int ... 74 more fields]\n",
       "delayedTrainingFlights: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Airline: string, label: int ... 74 more fields]\n",
       "downSampleFraction: Double = 0.2\n",
       "sampledOntimeTrainingFlights: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Airline: string, label: int ... 74 more fields]\n",
       "localTestingSampleFraction: Double = 0.1\n",
       "training: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Airline: string, label: int ... 74 more fields]\n",
       "resampledCounts: org.apache.spark.sql.DataFrame = [label: int, count: bigint]\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ontimeTrainingFlights = rawTraining.filter($\"label\"===0)\n",
    "println(s\"On time Training Flights: ${ontimeTrainingFlights.count()}\")\n",
    "\n",
    "val delayedTrainingFlights = rawTraining.filter($\"label\"===1)\n",
    "println(s\"Delayed Training Flights: ${delayedTrainingFlights.count()}\")\n",
    "\n",
    "//ontime:delayed approx 5:1 so take a random sample of size fifth of the ontime departures\n",
    "val downSampleFraction = 0.2\n",
    "val sampledOntimeTrainingFlights = ontimeTrainingFlights.sample(false, downSampleFraction)  \n",
    "\n",
    "println(s\"Down Sampled ontime Training Flights: ${sampledOntimeTrainingFlights.count()}\")\n",
    "\n",
    "//down sample resulting training set for the purposes of local testing\n",
    "val localTestingSampleFraction = 0.1\n",
    "//Concatenate rows of ontimeTrainingFlights and delayedTrainingFlights\n",
    "val training = (sampledOntimeTrainingFlights\n",
    "                .union(delayedTrainingFlights)\n",
    "                .sample(false, localTestingSampleFraction)\n",
    "                .cache())\n",
    "               \n",
    "val resampledCounts = training.groupBy(\"label\").count()\n",
    "println(\"proportion of lates (label=1) in the sample\")\n",
    "resampledCounts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contruct a Confusion Matrix for Model Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "getConfusionMatrix: (predictionDF: org.apache.spark.sql.DataFrame)Unit\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getConfusionMatrix(predictionDF: DataFrame): Unit = {\n",
    "    \n",
    "    println(\"========================Model Assessment Metrics==================================================\\n\")\n",
    "    // Define Binary Classification Evaluator\n",
    "    val binaryEval = new BinaryClassificationEvaluator().setLabelCol(\"label\").setRawPredictionCol(\"rawPrediction\")\n",
    "    // Run Evaluation.  The area under the ROC curve ranges from 0.5 and 1.0 with larger values indicative of better fit\n",
    "    println(s\"Area under ROC: ${binaryEval.setMetricName(\"areaUnderROC\").evaluate(predictionDF)}\")\n",
    "    // Define Multiclass Classification Evaluator\n",
    "    val multiEval = new MulticlassClassificationEvaluator().setLabelCol(\"label\").setPredictionCol(\"prediction\")\n",
    "    println(s\"Accuracy: ${multiEval.setMetricName(\"accuracy\").evaluate(predictionDF)}\")\n",
    "    println(s\"Weighted Precision: ${multiEval.setMetricName(\"weightedPrecision\").evaluate(predictionDF)}\")\n",
    "    println(s\"Weighted Recall: ${multiEval.setMetricName(\"weightedRecall\").evaluate(predictionDF)}\")\n",
    "    println(s\"F1: ${multiEval.setMetricName(\"f1\").evaluate(predictionDF)}\")\n",
    "\n",
    "    val TP = predictionDF.select(\"label\", \"prediction\").filter(\"label = 1 and prediction = 1\").count\n",
    "    val TN = predictionDF.select(\"label\", \"prediction\").filter(\"label = 0 and prediction = 0\").count\n",
    "    val FP = predictionDF.select(\"label\", \"prediction\").filter(\"label = 0 and prediction = 1\").count\n",
    "    val FN = predictionDF.select(\"label\", \"prediction\").filter(\"label = 1 and prediction = 0\").count\n",
    "    val total = predictionDF.select(\"label\").count.toDouble\n",
    "    // Unweighted Metrics\n",
    "    val accuracy    = (TP + TN) / total\n",
    "    val precision   = TP / (TP+FP).toDouble\n",
    "    val recall      = TP / (TP+FN).toDouble\n",
    "    val F1 = 2*precision*recall/(precision+recall)\n",
    "    println(s\"Accuracy: ${accuracy}\")\n",
    "    println(s\"Precision: ${precision}\")\n",
    "    println(s\"Recall: ${recall}\")\n",
    "    println(s\"F1: ${F1}\")\n",
    "\n",
    "    //predictionDF.select( $\"label\",$\"prediction\" cast \"Int\").orderBy(\"label\").groupBy(\"label\").pivot(\"prediction\",Seq(\"0\",\"1\")).count.show()\n",
    "\n",
    "    // Confusion matrix\n",
    "    printf(s\"\"\"|=================== Confusion Matrix ==========================\n",
    "           |##########| %-15s                     %-15s\n",
    "           |----------+----------------------------------------------------\n",
    "           |Actual = 0| %-15d                     %-15d\n",
    "           |Actual = 1| %-15d                     %-15d\n",
    "           |===============================================================\n",
    "         \"\"\".stripMargin, \"Predicted = 0\", \"Predicted = 1\", TN, FP, FN, TP)\n",
    "\n",
    "    println(\"\\n==================================================================================================\")\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conduct a PCA dimension reduction on weather columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|    weather_features|\n",
      "+--------------------+\n",
      "|[5.3,22.6,66.0,29...|\n",
      "|[3.9,7.4,43.0,21....|\n",
      "|[5.3,22.6,66.0,29...|\n",
      "|[5.8,23.1,69.0,29...|\n",
      "|[3.9,7.4,43.0,21....|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nonWeather: Array[String] = Array(Departing_Port, Arriving_Port, Airline, routeID, Date_Num, label, Year, Month_Num, Fuel_Price, Departing_Port_station_ID, Departing_Port_station_name, Arriving_Port_station_ID, Arriving_Port_station_name)\n",
       "columns: Array[String] = Array(Airline, label, Year, Month_Num, Fuel_Price, Departing_Port_station_ID, Departing_Port_station_name, Arriving_Port_station_ID, Arriving_Port_station_name, Mean_3pm_cloud_cover_oktas_Depart, Mean_3pm_dew_point_temperature_Degrees_C_Depart, Mean_3pm_relative_humidity_%_Depart, Mean_3pm_temperature_Degrees_C_Depart, Mean_3pm_wet_bulb_temperature_Degrees_C_Depart, Mean_3pm_wind_speed_km/h_Depart, Mean_9am_cloud_cover_okas_Depart, Mean_9am_dew_point_temperature_Degrees_C_Depart, Mean_9am_relative_humidity_%_Depart, Mean_9am_te..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// listing non-weather related columns and extracting as requried\n",
    "val nonWeather = Array(\"Departing_Port\", \"Arriving_Port\", \"Airline\", \"routeID\",\"Date_Num\", \"label\", \"Year\", \"Month_Num\", \"Fuel_Price\", \n",
    "                  \"Departing_Port_station_ID\", \"Departing_Port_station_name\", \"Arriving_Port_station_ID\", \n",
    "                  \"Arriving_Port_station_name\")\n",
    "val columns = flights.columns\n",
    "\n",
    "val weatherCols = columns.filter(!nonWeather.contains(_))\n",
    "\n",
    "\n",
    "// Using VectorAssembler to create single vector feature out of feature_cols\n",
    "\n",
    "val weather_assembler = new VectorAssembler()\n",
    "                        .setInputCols(weatherCols)\n",
    "                        .setOutputCol(\"weather_features\")\n",
    "\n",
    "val weathered_DF = weather_assembler.transform(flights)\n",
    "\n",
    "//weathered_DF.printSchema\n",
    "val weather_features_df = weathered_DF.select(\"weather_features\")\n",
    "weather_features_df.show(5)\n",
    "\n",
    "// creating standardarized features\n",
    "val std_scalar = new StandardScaler() \n",
    "                .setInputCol(\"weather_features\")\n",
    "                .setOutputCol(\"standard_features\")\n",
    "                .setWithStd(true)\n",
    "                .setWithMean(false)\n",
    "\n",
    "// implementing to PCA\n",
    "val pca = new PCA()\n",
    "          .setInputCol(\"standard_features\")\n",
    "          .setOutputCol(\"continuosWeatherFeatures\")\n",
    "          .setK(1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Flight Data Feature Processing Pipleline Stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "categoricalVariables: Array[String] = Array(Airline)\n",
       "categoricalIndexers: Array[org.apache.spark.ml.feature.StringIndexer] = Array(strIdx_f5c387684d1a)\n",
       "categoricalEncoders: Array[org.apache.spark.ml.feature.OneHotEncoder] = Array(oneHot_51df60eaa705)\n",
       "nonWeatherExplanatoryFields: Array[String] = Array(Airline_Vec, Fuel_Price, routeID, Date_Num)\n",
       "categorical_assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_ed97c79dffa1\n",
       "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_b42c6bb8597f\n",
       "featureProcessingStages: Array[org.apache.spark.ml.PipelineStage] = Array(vecAssembler_831770cfd00d, stdScal_0f2b457f131a, pca_fc5c281591b0, strIdx_f5c387684d1a, oneHot_51df60eaa705, vecAssembler_ed97c79dffa1, vecAssembler_b42c6bb8597f)\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Deal with Categorical Columns\n",
    "val categoricalVariables = Array(\"Airline\")\n",
    "val categoricalIndexers = categoricalVariables\n",
    "  .map(i => new StringIndexer().setInputCol(i).setOutputCol(i+\"_Index\"))\n",
    "val categoricalEncoders = categoricalVariables\n",
    "  .map(e => new OneHotEncoder().setInputCol(e + \"_Index\").setOutputCol(e + \"_Vec\"))\n",
    "\n",
    "\n",
    "// selecting non weather related features\n",
    "val nonWeatherExplanatoryFields = Array(\"Airline_Vec\", \"Fuel_Price\", \"routeID\", \"Date_Num\")\n",
    "\n",
    "// Assemble everything together to be (\"label\",\"features\") format\n",
    "val categorical_assembler = new VectorAssembler()\n",
    "                 .setInputCols(nonWeatherExplanatoryFields)\n",
    "                 //.setOutputCol(\"indexedFeatures\")\n",
    "                 .setOutputCol(\"nonWeatherFeatureVectors\")\n",
    "\n",
    "\n",
    "// creating final features\n",
    "\n",
    "val assembler = new VectorAssembler()\n",
    "                .setInputCols(Array(\"continuosWeatherFeatures\", \"nonWeatherFeatureVectors\"))\n",
    "                .setOutputCol(\"features\")\n",
    "\n",
    "///////////////////////////////////////////////////////////////////////////\n",
    "//   Define Feature Preprocessing Stages suitable for all candidate models  ///\n",
    "///////////////////////////////////////////////////////////////////////////\n",
    "val featureProcessingStages: Array[PipelineStage] = Array(weather_assembler, std_scalar, pca) ++ categoricalIndexers++categoricalEncoders++Array(categorical_assembler,assembler) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup a Cross Validated Logistic Regression Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lr: org.apache.spark.ml.classification.LogisticRegression = logreg_382506d1ca79\n",
       "lrParamGrid: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\tlogreg_382506d1ca79-elasticNetParam: 0.0,\n",
       "\tlogreg_382506d1ca79-regParam: 0.01,\n",
       "\tlogreg_382506d1ca79-threshold: 0.53,\n",
       "\tlogreg_382506d1ca79-tol: 1.0E-6\n",
       "})\n",
       "cv: org.apache.spark.ml.tuning.CrossValidator = cv_4c614d61b6da\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Define the Logistic Regression Estimator.\n",
    "val lr = new LogisticRegression()\n",
    "        .setFeaturesCol(assembler.getOutputCol)\n",
    "        //.setFeaturesCol(pca.getOutputCol)\n",
    "        .setLabelCol(\"label\")\n",
    "\n",
    "// Print out the parameters, documentation, and any default values.\n",
    "//println(s\"LogisticRegression parameters:\\n ${lr.explainParams()}\\n\")\n",
    "\n",
    "// We use a ParamGridBuilder to construct a grid of parameters to search over.\n",
    "val lrParamGrid = new ParamGridBuilder()\n",
    "  .addGrid(lr.regParam, Array(0.01))\n",
    "  .addGrid(lr.threshold, (for (i <- 53 to 53) yield i.toDouble / 100).toArray)\n",
    "  .addGrid(lr.tol, Array(0.000001))\n",
    "  .addGrid(lr.elasticNetParam, Array(0.0))\n",
    "  .build()\n",
    "\n",
    "// A CrossValidator requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
    "// Note that the evaluator here is a BinaryClassificationEvaluator and its default metric is areaUnderROC.\n",
    "val cv = new CrossValidator()\n",
    "  .setEstimator(new Pipeline().setStages(featureProcessingStages ++ Array(lr)))\n",
    "  .setEvaluator(new BinaryClassificationEvaluator)\n",
    "  .setEstimatorParamMaps(lrParamGrid)\n",
    "  .setNumFolds(10)  // Use 3+ in practice\n",
    "  //.setParallelism(2)  // Evaluate up to 2 parameter settings in parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Logistic Regression Model using Cross Validation Tuning for Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-08 10:08:46,268 WARN  [Executor task launch worker for task 4676] netlib.BLAS (BLAS.java:<clinit>(61)) - Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "2020-06-08 10:08:46,269 WARN  [Executor task launch worker for task 4676] netlib.BLAS (BLAS.java:<clinit>(61)) - Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "2020-06-08 10:08:46,858 WARN  [Thread-4] netlib.LAPACK (LAPACK.java:<clinit>(61)) - Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK\n",
      "2020-06-08 10:08:46,858 WARN  [Thread-4] netlib.LAPACK (LAPACK.java:<clinit>(61)) - Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK\n",
      "ElasticNetParam: 0.0\n",
      "Threshold: 0.53\n",
      "areaUnderCurve: 0.5747764894063114\n",
      "MaxFMeasure: 0.6676809765037431 & bestThreshold: 0.38238365779767464\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pipelineModel: org.apache.spark.ml.tuning.CrossValidatorModel = cv_4c614d61b6da\n",
       "bestModel: Option[org.apache.spark.ml.PipelineModel] = Some(pipeline_d8f7fd8f2a6a)\n",
       "ml: Option[org.apache.spark.ml.classification.LogisticRegressionModel] = Some(LogisticRegressionModel: uid = logreg_382506d1ca79, numClasses = 2, numFeatures = 13)\n",
       "lrModel: org.apache.spark.ml.classification.LogisticRegressionModel = LogisticRegressionModel: uid = logreg_382506d1ca79, numClasses = 2, numFeatures = 13\n",
       "lrModelSummary: org.apache.spark.ml.classification.BinaryLogisticRegressionSummary = org.apache.spark.ml.classification.BinaryLogisticRegressionTrainingSummaryImpl@4f297075\n",
       "fMeasure: org.apache.spark.sql.DataFrame = [threshold: double, F-Measure: double]\n",
       "maxFMeasure: Double = 0.6676809765037431\n",
       "bestThreshold: Doub..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Run cross-validation, and choose the best set of parameters.\n",
    "val pipelineModel = cv.fit(training)\n",
    "\n",
    "\n",
    "val bestModel = pipelineModel.bestModel match {\n",
    "  case pm: PipelineModel => Some(pm)\n",
    "  case _ => None\n",
    "}\n",
    "\n",
    "val ml = bestModel\n",
    "    .map(_.stages.collect { case ml: LogisticRegressionModel => ml })\n",
    "    .flatMap(_.headOption)\n",
    "\n",
    "// Get fitted logistic regression model\n",
    "val lrModel = ml.get.asInstanceOf[LogisticRegressionModel]\n",
    "\n",
    "//Get Coeffs of the Best Logistic Regression Model\n",
    "//println(s\"Intercept: ${lrModel.intercept}\")\n",
    "//println(s\"Coefficients: ${lrModel.coefficients}\")\n",
    "println(s\"ElasticNetParam: ${lrModel.getElasticNetParam}\")\n",
    "println(s\"Threshold: ${lrModel.getThreshold}\")\n",
    "\n",
    "\n",
    "val lrModelSummary = ml.get.summary.asInstanceOf[BinaryLogisticRegressionSummary]\n",
    "println(s\"areaUnderCurve: ${lrModelSummary.areaUnderROC}\")\n",
    "val fMeasure = lrModelSummary.fMeasureByThreshold\n",
    "val maxFMeasure = fMeasure.agg(\"F-Measure\" -> \"Max\").head().getDouble(0)\n",
    "val bestThreshold = fMeasure.where($\"F-Measure\" === maxFMeasure).select(\"threshold\").head().getDouble(0)\n",
    "println(s\"MaxFMeasure: $maxFMeasure & bestThreshold: $bestThreshold\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Logistic Regression Pipeline and Report on the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================================================\n",
      "+---------------------------------------------------------------------------+-------------------------+\n",
      "|feature                                                                    |logistic regression coeff|\n",
      "+---------------------------------------------------------------------------+-------------------------+\n",
      "|nonWeatherFeatureVectors_Airline_Vec_QantasLink                            |0.3791988268950665       |\n",
      "|nonWeatherFeatureVectors_Airline_Vec_Qantas                                |0.3581107404817141       |\n",
      "|nonWeatherFeatureVectors_Airline_Vec_Virgin Australia - ATR/F100 Operations|0.31936727784922636      |\n",
      "|nonWeatherFeatureVectors_Airline_Vec_Skywest                               |0.256363087172204        |\n",
      "|nonWeatherFeatureVectors_Airline_Vec_Jetstar                               |0.031460741155436935     |\n",
      "|nonWeatherFeatureVectors_Fuel_Price                                        |9.613348562595535E-4     |\n",
      "|nonWeatherFeatureVectors_routeID                                           |5.500348287484311E-4     |\n",
      "|(Intercept)                                                                |-0.011550506200724387    |\n",
      "|continuosWeatherFeatures_0                                                 |-0.12340112952256072     |\n",
      "|nonWeatherFeatureVectors_Airline_Vec_Virgin Australia                      |-0.13464105374780713     |\n",
      "|nonWeatherFeatureVectors_Airline_Vec_Tigerair Australia                    |-0.14944218933429376     |\n",
      "|nonWeatherFeatureVectors_Airline_Vec_Virgin Australia Regional Airlines    |-0.1919134744731417      |\n",
      "|nonWeatherFeatureVectors_Airline_Vec_Regional Express                      |-0.20646101130800382     |\n",
      "|nonWeatherFeatureVectors_Date_Num                                          |-0.6502496949701398      |\n",
      "+---------------------------------------------------------------------------+-------------------------+\n",
      "\n",
      "========================Model Assessment Metrics==================================================\n",
      "\n",
      "Area under ROC: 0.5553503067900188\n",
      "Accuracy: 0.6458381078975111\n",
      "Weighted Precision: 0.6839542692477381\n",
      "Weighted Recall: 0.6458381078975112\n",
      "F1: 0.66212313385451\n",
      "Accuracy: 0.6458381078975111\n",
      "Precision: 0.27169486215538846\n",
      "Recall: 0.36047176183301294\n",
      "F1: 0.30984971975973025\n",
      "=================== Confusion Matrix ==========================\n",
      "##########| Predicted = 0                       Predicted = 1  \n",
      "----------+----------------------------------------------------\n",
      "Actual = 0| 49423                               18598          \n",
      "Actual = 1| 12309                               6938           \n",
      "===============================================================\n",
      "         \n",
      "==================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lrPredictions: org.apache.spark.sql.DataFrame = [Airline: string, label: int ... 84 more fields]\n",
       "schema: org.apache.spark.sql.types.StructType = StructType(StructField(Airline,StringType,true), StructField(label,IntegerType,true), StructField(Year,IntegerType,true), StructField(Month_Num,StringType,true), StructField(Fuel_Price,DoubleType,true), StructField(Departing_Port_station_ID,StringType,true), StructField(Departing_Port_station_name,StringType,true), StructField(Arriving_Port_station_ID,StringType,true), StructField(Arriving_Port_station_name,StringType,true), StructField(Mean_3pm_cloud_cover_oktas_Depart,DoubleType,true), StructField(Mean_3pm_dew_point_temperature_Degrees_C_Depart,DoubleType,true), StructField(Mean_3pm_relative_humidity_%_Depart,DoubleType,true), StructField(Mea..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Test the model\n",
    "val lrPredictions = pipelineModel.transform(testing)\n",
    "\n",
    "// Get output schema of our fitted pipeline\n",
    "val schema = lrPredictions.schema\n",
    "// Extract the attributes of the input (features) column to our logistic regression model\n",
    "\n",
    "val featureAttrs = AttributeGroup.fromStructField(schema(lrModel.getFeaturesCol)).attributes.get\n",
    "\n",
    "val features = featureAttrs.map(_.name.get)\n",
    "\n",
    "// Add \"(Intercept)\" to list of feature names if the model was fit with an intercept\n",
    "val featureNames: Array[String] = if (lrModel.getFitIntercept) {\n",
    "  Array(\"(Intercept)\") ++ features\n",
    "} else {\n",
    "  features\n",
    "}\n",
    "\n",
    "// Get array of coefficients\n",
    "val lrModelCoeffs = lrModel.coefficients.toArray\n",
    "val coeffs = if (lrModel.getFitIntercept) \n",
    "        {lrModelCoeffs ++ Array(lrModel.intercept)} \n",
    "    else \n",
    "        {lrModelCoeffs}\n",
    "\n",
    "// Print feature names & coefficients together\n",
    "//println(\"Coefficient   Feature\")\n",
    "println(\"==============================================================================\")\n",
    "\n",
    "val lrFeatureContributions = sc.parallelize(featureNames.zip(coeffs))\n",
    "    .toDF(\"feature\", \"logistic regression coeff\")\n",
    "    .sort(desc(\"logistic regression coeff\"))\n",
    "val numRows = lrFeatureContributions.count().toInt\n",
    "lrFeatureContributions.show(numRows,truncate=false)\n",
    "//Save Coefficients to File for the Group Report\n",
    "lrFeatureContributions.coalesce(1) //Join all partitions into one file\n",
    "      .write\n",
    "      .option(\"header\",\"true\")\n",
    "      .option(\"sep\",\",\")\n",
    "      .mode(\"overwrite\")\n",
    "      .csv(\"lrCoeffs.csv\")\n",
    "//lrPredictions.select(\"prediction\", \"label\", \"features\").show(20)\n",
    "getConfusionMatrix(lrPredictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store The Best CV Trained Logistic Model to the hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `hdfs://localhost:9000/tmp/flightDelayModel_*': No such file or directory\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -rm    -r  hdfs://localhost:9000/tmp/flightDelayModel_*\n",
    "! hadoop fs -mkdir -p  hdfs://localhost:9000/tmp/flightDelayModel__out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "//Persist the Model to the hdfs\n",
    "pipelineModel\n",
    "    .write\n",
    "    .overwrite()\n",
    "    .save(\"hdfs://localhost:9000/tmp/flightDelayModel__out\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------+-----+----------+\n",
      "|features                                                    |label|prediction|\n",
      "+------------------------------------------------------------+-----+----------+\n",
      "|(13,[0,4,10,11,12],[11.78978030458197,1.0,2.78,103.0,184.0])|1    |1.0       |\n",
      "|(13,[0,4,10,11,12],[11.78978030458197,1.0,2.78,103.0,184.0])|1    |1.0       |\n",
      "+------------------------------------------------------------+-----+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "results: org.apache.spark.sql.DataFrame = [features: vector, label: int ... 1 more field]\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Check the stored model, by reading it back in, and running a prediciton\n",
    "val results: DataFrame = CrossValidatorModel\n",
    ".load(\"hdfs://localhost:9000/tmp/flightDelayModel__out\")\n",
    ".transform(testing)\n",
    ".select(\n",
    "    col(\"features\"),\n",
    "    col(\"label\"),\n",
    "    col(\"prediction\")\n",
    ")\n",
    "\n",
    "results.show(2,truncate = false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
